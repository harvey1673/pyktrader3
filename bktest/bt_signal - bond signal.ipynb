{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6453acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['PY3_PROD'] = '1'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.system('kinit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8366f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib\n",
    "from pycmqlib3.utility import dbaccess, dataseries, misc\n",
    "from pycmqlib3.analytics.tstool import *\n",
    "from pycmqlib3.analytics.btmetrics import *\n",
    "from pycmqlib3.analytics.backtest_utils import *\n",
    "from pycmqlib3.strategy.signal_repo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113b4f2",
   "metadata": {},
   "source": [
    "# load historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfd4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from misc_scripts.update_fut_prices import load_saved_fut\n",
    "tday = datetime.date(2025, 3, 14)\n",
    "\n",
    "df = load_saved_fut(tday, freq='d')\n",
    "#df = load_cnc_fut(tday, type='cnc')\n",
    "\n",
    "start_date = df.index[0]\n",
    "end_date = tday\n",
    "\n",
    "cdates = pd.date_range(start=start_date, end=tday, freq='D')\n",
    "bdates = pd.bdate_range(start=start_date, end=end_date, freq='C', holidays=misc.CHN_Holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cedba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_pairs = [\n",
    "    ('T', 'TF'), ('TF', 'T'), ('TS', 'TF'), ('T', 'TL'),\n",
    "]\n",
    "\n",
    "roll_win = 244\n",
    "beta_ret_dict = {}\n",
    "betas_dict = {}\n",
    "for trade_asset, index_asset in asset_pairs:\n",
    "    asset_df = df[[(index_asset+'c1', 'close'), (trade_asset+'c1', 'close')]].copy(deep=True).droplevel([1], axis=1)\n",
    "    asset_df = asset_df.dropna(subset=[trade_asset+'c1']).ffill()\n",
    "\n",
    "    for asset in asset_df:\n",
    "        asset_df[f'{asset}_pct'] = asset_df[asset].pct_change().rolling(1).mean()\n",
    "    asset_df['beta'] = asset_df[f'{index_asset}c1_pct'].rolling(244).cov(asset_df[f'{trade_asset}c1_pct'])/asset_df[f'{index_asset}c1_pct'].rolling(roll_win).var()\n",
    "    key = '_'.join([trade_asset, index_asset])\n",
    "    asset_df[key] = asset_df[trade_asset+'c1'].pct_change() - asset_df['beta'] * asset_df[index_asset+'c1'].pct_change().fillna(0)\n",
    "    beta_ret_dict[key] = asset_df[key].dropna()\n",
    "    betas_dict[key] = asset_df['beta']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effee2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_df = load_fun_data(tday)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce1327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_dict = {}\n",
    "\n",
    "vol_win = 20\n",
    "product_list = list(set([col[:-2] for col in df.columns.get_level_values(0).unique()]))\n",
    "\n",
    "for asset in product_list:    \n",
    "    spot_dict[f'{asset}_px'] = df[(asset+'c1', 'close')]\n",
    "    spot_dict[f'{asset}_logret'] = np.log(df[(asset+'c1', 'close')]).dropna().diff()\n",
    "    spot_dict[f'{asset}_pctchg'] = np.log(df[(asset+'c1', 'close')]).dropna().pct_change()        \n",
    "    if (asset+'c2', 'close') not in df.columns:\n",
    "        print(asset)\n",
    "        continue\n",
    "    spot_dict[f'{asset}_ryield'] = (np.log(df[(asset+'c1', 'close')]) - np.log(df[(asset+'c2', 'close')]) - \n",
    "                                  df[(asset+'c1', 'shift')] + df[(asset+'c2', 'shift')])/(df[(asset+'c2', 'expiry')] - df[(asset+'c1', 'expiry')]).dt.days*365.0 + spot_df['shibor_1m'].dropna().ewm(1).mean()/100\n",
    "    spot_dict[f'{asset}_logret2'] = np.log(df[(asset+'c2', 'close')]).dropna().diff()\n",
    "    spot_dict[f'{asset}_basmom'] =spot_dict[f'{asset}_logret'] - spot_dict[f'{asset}_logret2']        \n",
    "    spot_dict[f'{asset}_pctvol'] = spot_dict[f'{asset}_pctchg'].dropna().rolling(vol_win).std()\n",
    "    spot_dict[f'{asset}_basmom5'] = spot_dict[f'{asset}_basmom'].dropna().rolling(5).sum()\n",
    "    spot_dict[f'{asset}_basmom10'] = spot_dict[f'{asset}_basmom'].dropna().rolling(10).sum()\n",
    "    spot_dict[f'{asset}_basmom20'] = spot_dict[f'{asset}_basmom'].dropna().rolling(20).sum()\n",
    "    spot_dict[f'{asset}_basmom60'] = spot_dict[f'{asset}_basmom'].dropna().rolling(60).sum()\n",
    "\n",
    "keys = [key for key in spot_df.columns if key not in spot_dict]\n",
    "spot_df = pd.concat([spot_df[keys], pd.DataFrame(spot_dict)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c825135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178242e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "def create_holiday_window_series(index, holidays, pre_days, post_days):\n",
    "    chn_bday = CustomBusinessDay(holidays=pd.to_datetime(CHN_Holidays))\n",
    "    series = pd.Series(0, index=index)        \n",
    "    for holiday in holidays:        \n",
    "        start = holiday + pre_days * chn_bday\n",
    "        end = holiday + post_days * chn_bday               \n",
    "        series.loc[(series.index >= start) & (series.index <= end)] = 1\n",
    "    return series\n",
    "\n",
    "start_date = datetime.datetime(2014, 5, 1)\n",
    "\n",
    "# Get the current year and month\n",
    "current_date = datetime.datetime.now()\n",
    "latest_contract = current_date.strftime(\"%Y%m\")  # Latest contract in YYYYMM format\n",
    "\n",
    "# Generate contract months from May 2020 to the latest contract\n",
    "contract_months = []\n",
    "while start_date.strftime(\"%Y%m\") <= latest_contract:\n",
    "    contract_months.append(int(start_date.strftime(\"%Y%m\")))\n",
    "    # Move to the next month\n",
    "    start_date += datetime.timedelta(days=32)\n",
    "    start_date = start_date.replace(day=1)  # Ensure we stay at the first day of the month\n",
    "\n",
    "opt_expiries = [misc.get_opt_expiry(\"i\"+str(cont)[-4:], cont) for cont in contract_months]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a148aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asset in empiric_assets:\n",
    "    spot_df[f\"{asset}_reaction\"] = np.sign(df[(asset+'c1', 'close')] - df[(asset+'c1', 'settle')])*(df[(asset+'c1', 'close')].shift(-1)-df[(asset+'c1', 'close')]) #/df[(asset+'c1', 'close')].diff().std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f293424",
   "metadata": {},
   "source": [
    "# feature study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_win=20\n",
    "pnl_tenors = ['6m', '1y', '2y', '3y', '4y', '5y', '6y', '7y', '8y', '9y', '10y']\n",
    "insample = \"2026-01-01\"\n",
    "\n",
    "empiric_assets = [\n",
    "    'T', 'TF', 'TS', 'TL',\n",
    "    #'rb', 'hc', 'j', 'jm', 'i', 'v', 'FG', 'SM', 'SF', 'SA', \n",
    "    #'cu', 'al', 'zn', 'ni', 'sn', 'ss', 'pp', 'sc', 'lu'\n",
    "]\n",
    "\n",
    "traded_price = 'a1535'\n",
    "df_pxchg = pd.DataFrame(index=df.index)\n",
    "\n",
    "for asset in empiric_assets:\n",
    "    if '_' in asset:\n",
    "        df_pxchg[asset] = beta_ret_dict[asset].dropna()[:insample]\n",
    "    elif '-' in asset:\n",
    "        for sub_asset in asset.split('-'):\n",
    "            if sub_asset not in empiric_assets:\n",
    "                df_pxchg[sub_asset] = df[(sub_asset+'c1', traded_price)].dropna().pct_change()[:insample]                 \n",
    "    else:\n",
    "        if asset in [\"T\", \"TF\", \"TS\", \"TL\", \"IF\", \"IH\", \"IC\", \"IM\"]:\n",
    "            if traded_price in ['n305', 'n310', 'n315', 'n450', 'a1505', 'a1510', 'a1515']:\n",
    "                ts_px = df[(asset+'c1', 'a1535')]\n",
    "            else:\n",
    "                ts_px = df[(asset+'c1', traded_price)]\n",
    "            flag = ts_px.isna()\n",
    "            ts_px.loc[flag] = df[(asset+'c1', 'd_twap')].loc[flag]\n",
    "        else:\n",
    "            ts_px = df[(asset+'c1', traded_price)]\n",
    "            if traded_price in ['n305', 'n310', 'n315', 'n450']:\n",
    "                flag = ts_px.isna()\n",
    "                ts_px.loc[flag] = df[(asset+'c1', 'a1505')].loc[flag]\n",
    "            elif traded_price in ['n_twap']:\n",
    "                flag = ts_px.isna()\n",
    "                ts_px.loc[flag] = df[(asset+'c1', 'd_twap')].loc[flag]        \n",
    "                \n",
    "        df_pxchg[asset] = ts_px.dropna().pct_change()[:insample]        \n",
    "vol_df = df_pxchg.rolling(vol_win).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df2 = pd.DataFrame(index=df.index)\n",
    "for asset in df_pxchg.columns:\n",
    "    vol_df2[asset] = robust_vol_calc(df_pxchg[asset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31042d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_df['pmi_steel_order_inv_ratio'] = spot_df['pmi_cn_steel_new_order']/spot_df['pmi_cn_steel_inv']\n",
    "spot_df['pmi_steel_order_prod_ratio'] = spot_df['pmi_cn_steel_new_order']/spot_df['pmi_cn_steel_prod']\n",
    "spot_df['pmi_steel_order_rminv_ratio'] = spot_df['pmi_cn_steel_new_order']/spot_df['pmi_cn_steel_rm_inv']\n",
    "\n",
    "spot_df['pmi_order_rminv_ratio'] = spot_df['pmi_cn_manu_new_order']/spot_df['pmi_cn_manu_rm_inv']\n",
    "spot_df['pmi_order_purchase_ratio'] = spot_df['pmi_cn_manu_new_order']/spot_df['pmi_cn_manu_purchase']\n",
    "\n",
    "spot_df['pmi_lgsc_order_inv_ratio'] = spot_df['pmi_lgsc_steel_tot_order']/spot_df['pmi_lgsc_steel_inv']\n",
    "spot_df['pmi_lgsc_purchase_inv_ratio'] = spot_df['pmi_lgsc_steel_purchase_exp']/spot_df['pmi_lgsc_steel_inv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfdcbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff='2010-01-01'\n",
    "\n",
    "signal_cap = [-2, 2]\n",
    "chg_func = 'diff'\n",
    "bullish = False\n",
    "vol_win = 24\n",
    "\n",
    "by_asset = False\n",
    "\n",
    "signal_func = 'zscore'\n",
    "param_rng = [48, 60, 1]\n",
    "#feature = 'exch_warrant'\n",
    "#feature = \"sm_margin_north\"\n",
    "#feature = \"sf_neimeng_margin\"\n",
    "#feature = \"cnh_cny_spd2\"\n",
    "#feature = 'base_sw_csi500_ret'\n",
    "#feature = 'glass_sw_csi500_ret'\n",
    "#feature = 'sf_neimeng_cost'\n",
    "#feature = 'mn_44_gabon_tj'\n",
    "#feature = \"phycarry\"\n",
    "# feature = 'rebar_sales_inv_ratio'\n",
    "#feature = \"pmi_cn_cons_bus_exp\"\n",
    "#'ni_npi_10-15_sh' #'ni_nis_cjb_spot' # 'ni_mhp_34_ports' #\"ni_1.5conc_spot_rz\" #'ni_npi_10-15_sh'\n",
    "#feature = 'ppi_cpi_mom_spd' #'pmi_cn_steel_all' #\n",
    "feature=\"pmi_cn_manu_all\"\n",
    "#feature='fxbasket_cumret'\n",
    "\n",
    "freq=''\n",
    "\n",
    "signal_df = pd.DataFrame(index=df_pxchg.index)\n",
    "\n",
    "for asset in empiric_assets:\n",
    "    #feature_ts = df_pxchg[asset].cumsum()\n",
    "    #feature_ts = df[(asset+'c1', 'close')].dropna() #.pct_change() \n",
    "    if by_asset:\n",
    "        asset_feature = f\"{asset}_{feature}\"\n",
    "    else:\n",
    "        asset_feature = feature\n",
    "    feature_ts = spot_df[asset_feature].dropna() #.rolling(12).sum()\n",
    "    #feature_ts = feature_ts.diff(12)\n",
    "    #feature_ts = feature_ts.ewm(1).mean()\n",
    "    #feature_ts.index = feature_ts.index + pd.DateOffset(days=9) + chn_bday * 1\n",
    "    #feature_ts = np.log(feature_ts)\n",
    "    #feature_ts = feature_ts.rolling(100).sum()\n",
    "    #feature_ts = (1+stock_pct_chg[[\"SPY.P\"]].mean(axis=1)).cumprod()\n",
    "    #feature_ts = (1+stock_pct_chg[['SPY.P']].mean(axis=1)).cumprod()\n",
    "    #feature_ts = spot_df[asset_feature].ffill().reindex(index=df_pxchg.index)\n",
    "    #feature_ts = beta_ret_dict[asset].dropna().cumsum()\n",
    "    #feature_ts = spot_df[feature].ffill().reindex(index=pd.date_range(start=df.index[0], end=df.index[-1], freq=freq)).ffill().dropna()    \n",
    "    #feature_ts = df[(asset, 'c1', 'close')].dropna()\n",
    "    #ticker, param_rng = feature_map[asset]    \n",
    "    #feature_ts = (spot_df[beta_feature_map[asset][0]]/spot_df[beta_feature_map[asset][1]]).dropna()\n",
    "    #signal_ts = (feature_ts.ewm(8).mean() - feature_ts.ewm(30).mean())/feature_ts.diff().ewm(30).std()\n",
    "#     feature_ts = beta_residual(\n",
    "#         spot_df[feature].dropna(),\n",
    "#         df_pxchg[asset].dropna().cumsum(),\n",
    "#         beta_win=120, \n",
    "#         chg_func='diff') \n",
    "#     feature_ts = beta_residual(\n",
    "#         spot_df[beta_feature_map[asset][0]].dropna(),\n",
    "#         spot_df[beta_feature_map[asset][1]].dropna(),\n",
    "#         beta_win=250, \n",
    "#         chg_func='pct_change') \n",
    "    #feature_ts = np.log(feature_ts)\n",
    "#     if asset in ['cu', 'al']:\n",
    "#         #param_rng = [1, 2, 1]\n",
    "#         feature_ts = feature_ts.rolling(20).mean()\n",
    "#     else:\n",
    "#         #param_rng = [1, 2, 1]\n",
    "#         feature_ts = feature_ts.rolling(2).mean()\n",
    "    #feature_ts = feature_ts.reindex(index=cdates).ffill().reindex(index=bdates)\n",
    "    #feature_ts = feature_ts.ewm(10).mean()\n",
    "    #feature_ts = yoy_generic(feature_ts, label_func=lunar_label, group_col='label_day', func=chg_func)[asset_feature]    \n",
    "    signal_ts = calc_conv_signal(feature_ts, signal_func=signal_func, param_rng=param_rng, signal_cap=signal_cap, vol_win=vol_win)\n",
    "    #signal_ts = feature_ts\n",
    "    #signal_ts = 4*signal_ts - 3*signal_ts.shift(1)\n",
    "    #signal_ts = np.sign(signal_ts)  \n",
    "    #signal_ts = conv_ewm(feature_ts, [2, 6, 2], [8, 32, 4], vol_win=60)    \n",
    "    #signal_ts = zscore_roll(feature_ts, 120)\n",
    "    #signal_ts = ewmac(feature_ts, 8, 16, vol_win=0)\n",
    "    #signal_ts = pd.Series(1, index=df_pxchg.index)\n",
    "    #signal_ts.loc[signal_ts.index.month.isin([1, 2, 3, 4, 5, 6, 7, 12])] = 1\n",
    "    #signal_ts.loc[signal_ts.index.day.isin(range(16, 32))] += 1\n",
    "    #signal_ts = zscore_roll(feature_ts, 60)\n",
    "    #signal_ts = signal_hysteresis(signal_ts, 1.2, 0.6, False)\n",
    "    #signal_ts = seasonal_score(feature_ts.to_frame(), backward=10, forward=10, rolling_years=5, min_obs=10)\n",
    "    # signal_ts = seasonal_score(feature_ts.to_frame(), backward=15, forward=15, rolling_years=3, min_obs=30)\n",
    "    #signal_ts = create_holiday_window_series(df.index, opt_expiries, 1, 3) # - create_holiday_window_series(df.index, opt_expiries, 3, 4)\n",
    "    if not bullish:\n",
    "        signal_ts = -signal_ts    \n",
    "    \n",
    "    signal_ts = signal_ts.reindex(index=cdates).ffill().reindex(index=df_pxchg.index)\n",
    "    #signal_ts = signal_ts.ewm(3).mean()    \n",
    "    #signal_ts.loc[signal_ts.index.month.isin([1, 2])] = 0\n",
    "    #signal_ts = signal_hump(signal_ts, 0.2)\n",
    "    if '-' in asset:\n",
    "        sub_assets = asset.split('-')\n",
    "        if sub_assets[0] in signal_df.columns:\n",
    "            signal_df[sub_assets[0]] += signal_ts\n",
    "        else:\n",
    "            signal_df[sub_assets[0]] = signal_ts\n",
    "        if sub_asset[1] in signal_df.columns:\n",
    "            signal_df[sub_assets[1]] -= signal_ts\n",
    "        else:\n",
    "            signal_df[sub_assets[1]] = -signal_ts            \n",
    "    else:\n",
    "        signal_df[asset] = signal_ts\n",
    "\n",
    "#signal_df = xs_demean(signal_df)\n",
    "#signal_df = signal_df + xs_demean(signal_df)\n",
    "#signal_df = signal_buffer(signal_df, 0.1)\n",
    "\n",
    "holding = generate_holding_from_signal(signal_df, vol_df, risk_scaling=1.0, asset_scaling=False)\n",
    "\n",
    "bt_metrics = MetricsBase(holdings=holding[empiric_assets][cutoff:],\n",
    "                         returns=df_pxchg[empiric_assets][cutoff:], \n",
    "                         shift_holdings=1)\n",
    "trading_cost = dict([(asset, 0.6e-4) if asset in [\"T\", \"TF\", \"TS\", \"TL\"] else (asset, 2e-4) for asset in holding.columns])\n",
    "\n",
    "bt_metrics_w_cost = MetricsBase(holdings=holding[empiric_assets][cutoff:],\n",
    "                                returns=df_pxchg[empiric_assets][cutoff:], \n",
    "                                shift_holdings=1,\n",
    "                                cost_dict=trading_cost)\n",
    "\n",
    "pnl_stats = bt_metrics.calculate_pnl_stats(shift=0, use_log_returns=False, tenors=pnl_tenors)\n",
    "pnl_stats_w_cost = bt_metrics_w_cost.calculate_pnl_stats(shift=0, use_log_returns=False, tenors=pnl_tenors)\n",
    "\n",
    "print(\"SR after cost:\\n\", pnl_stats_w_cost['sharpe'])\n",
    "print(pnl_stats_w_cost['asset_sharpe_stats'])\n",
    "print(\"Turnover: \\n%s\\nPNL per trade:\\n%s\\n\" % (pnl_stats_w_cost['turnover'], pnl_stats_w_cost['pnl_per_trade']))\n",
    "\n",
    "print(\"SR before cost:\\n\", pnl_stats['sharpe'])\n",
    "print(pnl_stats['asset_sharpe_stats'])\n",
    "print(\"Turnover: \\n%s\\nPNL per trade:\\n%s\\n\" % (pnl_stats['turnover'], pnl_stats['pnl_per_trade']))\n",
    "\n",
    "iplot(pnl_stats_w_cost['portfolio_cumpnl'], title='portfolio pnl with cost')\n",
    "iplot(pnl_stats_w_cost['asset_cumpnl'], title='asset pnl with cost')\n",
    "\n",
    "iplot(pnl_stats['portfolio_cumpnl'], title='portfolio pnl wo cost')\n",
    "iplot(pnl_stats['asset_cumpnl'], title='asset pnl wo cost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f8f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = pnl_stats[\"portfolio_pnl\"]['total']\n",
    "pnl = pnl[pnl.abs()>0].to_frame()\n",
    "print(\"zscore=%s\" % (pnl.mean()/pnl.std()*np.sqrt(len(pnl)/len(df_pxchg.loc[pnl.index[0]:pnl.index[-1]]))*16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_perf_by_tenors(pnl['total'],tenors=['1y', '2y', '3y', '5y', '7y', '10y'], metric='sharpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_lag_config = {\n",
    "    'll_left': -20,\n",
    "    'll_right': 120,\n",
    "    'll_spacing': 5,\n",
    "    'll_sub_win': [(datetime.date(2008, 1, 1), datetime.date(2018, 12, 31)), \n",
    "                   (datetime.date(2019, 1, 1), datetime.date(2024, 12, 31)),],\n",
    "}\n",
    "\n",
    "ll_keys = ['fullsample'] + ['%s:%s' % (sd.strftime('%Y-%b-%d'), ed.strftime('%Y-%b-%d')) for sd, ed in lead_lag_config['ll_sub_win']]\n",
    "\n",
    "#tilt_timing = bt_metrics.tilt_timing(tilt_rolling_window=1*244) # default 3 years  tilt_rolling_window = 3 * 244 \n",
    "\n",
    "seasonal_pnl = bt_metrics.seasonal_pnl()\n",
    "cumpnl = seasonal_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "print('seasonal sharpe stats\\n', seasonal_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('monthly pnl')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "monthday_pnl = bt_metrics.monthday_pnl()\n",
    "cumpnl = monthday_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "#print('monthday sharpe stats\\n', monthday_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('monthday pnl')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "week_pnl = bt_metrics.week_pnl()\n",
    "cumpnl = week_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "#print('week sharpe stats\\n', week_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('weekday pnl')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "annual_pnl = bt_metrics.annual_pnl()\n",
    "cumpnl = annual_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "#print('annual sharpe stats\\n', annual_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('annual pnl')\n",
    "plt.show()\n",
    "\n",
    "annual_pnl['cumlog_pnl'].mean(axis=1).plot()\n",
    "plt.grid()\n",
    "plt.title('annual averaged profile')\n",
    "plt.show()\n",
    "\n",
    "# turnover = bt_metrics.turnover()\n",
    "# print(turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91824919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2f583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_win=20\n",
    "pnl_tenors = ['6m', '1y', '2y', '3y', '4y', '5y', '6y', '7y', '8y', '9y', '10y']\n",
    "insample = \"2026-01-01\"\n",
    "\n",
    "empiric_assets = [\n",
    "#     'cu', 'zn', 'al', 'ni', 'ss',\n",
    "#     'rb', 'hc', 'SM', 'SF', 'FG', 'v',\n",
    "#     'y', 'OI', 'm', 'RM',\n",
    "#     'l', 'MA', 'pp', 'TA', 'eg',\n",
    "#     'au', 'ag', \n",
    "    'T', 'TF', 'TL',\n",
    "    #'TL', 'TS',\n",
    "#     'IF', 'IC', 'IH'\n",
    "]\n",
    "\n",
    "traded_price = 'a1535'\n",
    "df_pxchg = pd.DataFrame(index=df.index)\n",
    "\n",
    "for asset in empiric_assets:\n",
    "    if '_' in asset:\n",
    "        df_pxchg[asset] = beta_ret_dict[asset].dropna()[:insample]\n",
    "    elif '-' in asset:\n",
    "        for sub_asset in asset.split('-'):\n",
    "            if sub_asset not in empiric_assets:\n",
    "                df_pxchg[sub_asset] = df[(sub_asset+'c1', traded_price)].dropna().pct_change()[:insample]                 \n",
    "    else:\n",
    "        ts_px = df[(asset+'c1', traded_price)]\n",
    "        if traded_price in ['n305', 'n310', 'n315', 'n450']:\n",
    "            flag = ts_px.isna()\n",
    "            ts_px.loc[flag] = df[(asset+'c1', 'a1505')].loc[flag]\n",
    "        df_pxchg[asset] = ts_px.dropna().pct_change()[:insample]        \n",
    "vol_df = df_pxchg.rolling(vol_win).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b0e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_setup = {\n",
    "    'bond_mr_st_qtl': [['T', 'TF'],\n",
    "                       ['px', 'qtl', [2, 5, 1], 'df1', 'pct_change', False, '', \"ema10|buf0.1\", 120, [-2,2]]],\n",
    "    'bond_tf_lt_qtl': [['T', 'TF', 'TL'],\n",
    "                       ['px', 'qtl', [230, 250, 2], '', '', True, '', \"buf0.1\", 120, [-2,2]]],\n",
    "    'bond_tf_st_eds': [['T', 'TF', 'TL', \"TS\"],\n",
    "                       ['px', 'ema_dff_sgn', [20, 40, 2], '', '', True, '', \"ema5\", 120, [-2,2]]],\n",
    "    'bond_carry_ma': [['T', 'TL'],\n",
    "                      ['ryield', 'ma', [1, 2, 1], '', '', True, '', \"\", 120, [-2,2]]],\n",
    "    \"bond_fxbasket_zs\": [['T', 'TF', 'TL', \"TS\"],\n",
    "                    ['fxbasket_cumret', 'zscore', [480, 520, 2], '', '', True, '', '', 120, [-2,2]]],        \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619f9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9303e262",
   "metadata": {},
   "source": [
    "standard signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff='2016-01-01'\n",
    "\n",
    "feature_names = [\n",
    "    ['bond_mr_st_qtl', 2.406], # \n",
    "    ['bond_tf_lt_qtl', 0.1], # 0.38\n",
    "    ['bond_carry_ma', 0.05], # 0.22\n",
    "    ['bond_tf_st_eds', 0.138],\n",
    "    [\"bond_fxbasket_zs\", 0.097],\n",
    "]\n",
    "\n",
    "signal_dict = {}\n",
    "signal_df = pd.DataFrame(0, columns=empiric_assets, index=cdates)\n",
    "\n",
    "for feature_name, weight in feature_names:\n",
    "    signal_assets = feature_setup[feature_name][0]\n",
    "    feature, signal_func, param_rng, proc_func, chg_func, bullish, freq, post_func, vol_win, signal_cap = feature_setup[feature_name][1] \n",
    "    if feature in ['sinv', 'base_phybas', 'prem_bonded_warrant', 'prem_bonded_cif', \n",
    "                   'tc', 'phycarry', 'ryield', 'basmom5', 'basmom10', 'basmom20', 'basmom40', 'basmom60', 'basmom120', \n",
    "                   'base_inv', 'inv_shfe_d', 'inv_lme_total', 'inv_exch_d', 'px']:\n",
    "        sig_df = pd.DataFrame(0, columns=empiric_assets, index=cdates)\n",
    "        for asset in empiric_assets:\n",
    "            if feature == 'base_inv':\n",
    "                asset_feature = base_inv.get(asset, f\"{asset}_{feature}\")\n",
    "            else:\n",
    "                asset_feature = f\"{asset}_{feature}\"\n",
    "            if (asset not in signal_assets) or (asset_feature not in spot_df.columns):\n",
    "                sig_df[asset] = np.nan\n",
    "                continue\n",
    "            if feature in ['base_phybas']:\n",
    "                if asset in ['cu', 'al']: \n",
    "                    proc_func = 'sma20'\n",
    "                else:\n",
    "                    proc_func = 'sma2'\n",
    "            signal_ts = calc_funda_signal(spot_df, asset_feature, signal_func, param_rng,\n",
    "                                          proc_func=proc_func, chg_func=chg_func, bullish=bullish,\n",
    "                                          freq=freq, signal_cap=signal_cap, bdates=bdates,\n",
    "                                          post_func=post_func, vol_win=vol_win)\n",
    "            sig_df[asset] = signal_ts\n",
    "        sig_df = sig_df.reindex(index=cdates).ffill().reindex(index=df_pxchg.index)\n",
    "        if \"xdemean\" in feature_name:\n",
    "            sig_df = xs_demean(sig_df)\n",
    "        elif \"xscore\" in feature_name:\n",
    "            sig_df = xs_score(sig_df)\n",
    "        elif \"xrank\" in feature_name:\n",
    "            sig_df = xs_rank(sig_df, 0.2)\n",
    "\n",
    "    else:\n",
    "        sig_ts = calc_funda_signal(spot_df, feature, signal_func, param_rng,\n",
    "                                   proc_func=proc_func, chg_func=chg_func, bullish=bullish,\n",
    "                                   freq=freq, signal_cap=signal_cap, bdates=bdates,\n",
    "                                   post_func=post_func, vol_win=vol_win)\n",
    "        sig_df = pd.DataFrame(columns=empiric_assets, index=cdates)\n",
    "        for asset in empiric_assets:\n",
    "            if asset in signal_assets:\n",
    "                sig_df[asset] = sig_ts\n",
    "            else:\n",
    "                sig_df[asset] = 0\n",
    "        sig_df = sig_df.reindex(index=cdates).ffill().reindex(index=df_pxchg.index)\n",
    "    last_func = post_func.split('|')[-1]\n",
    "    if 'buf' in last_func:\n",
    "        buffer_size = float(last_func[3:])\n",
    "        sig_df = signal_buffer(sig_df, buffer_size)\n",
    "    elif 'bfc' in last_func:\n",
    "        buffer_size = float(last_func[3:])\n",
    "        sig_df = signal_cost_optim(sig_df, \n",
    "                                   buffer_size, \n",
    "                                   vol_df, \n",
    "                                   cost_dict = dict([(asset, 1e-4) for asset in empiric_assets]))\n",
    "    sig_df = sig_df.reindex(index=cdates).ffill().reindex(index=df_pxchg.index).fillna(0)\n",
    "    signal_dict[feature_name] = sig_df * weight\n",
    "    signal_df = signal_df + signal_dict[feature_name]\n",
    "    \n",
    "signal_dict['combo'] = signal_df.dropna().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65a039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl_df = pd.DataFrame(index=df_pxchg.index)\n",
    "for signal_name in signal_dict:\n",
    "    signal_df = signal_dict[signal_name]\n",
    "    holding = generate_holding_from_signal(signal_df, vol_df, risk_scaling=1, asset_scaling=False)    \n",
    "    \n",
    "    bt_metrics = MetricsBase(holdings=holding[empiric_assets][cutoff:],\n",
    "                             returns=df_pxchg[empiric_assets][cutoff:], \n",
    "                             shift_holdings=1)\n",
    "\n",
    "    bt_metrics_w_cost = MetricsBase(holdings=holding[empiric_assets][cutoff:],\n",
    "                                    returns=df_pxchg[empiric_assets][cutoff:], \n",
    "                                    shift_holdings=1,\n",
    "                                    cost_dict=simple_cost(holding.columns, trd_cost=1e-4))\n",
    "\n",
    "    pnl_stats = bt_metrics.calculate_pnl_stats(shift=0, use_log_returns=False, tenors=pnl_tenors)\n",
    "    pnl_stats_w_cost = bt_metrics_w_cost.calculate_pnl_stats(shift=0, use_log_returns=False, tenors=pnl_tenors)\n",
    "    pnl_df[signal_name] = pnl_stats_w_cost['portfolio_pnl']['total']\n",
    "    \n",
    "    print(\"signal=%s\\n\" % signal_name)\n",
    "    print(\"SR after cost:\\n\", pnl_stats_w_cost['sharpe'])\n",
    "    print(pnl_stats_w_cost['asset_sharpe_stats'])\n",
    "    print(\"Turnover: \\n%s\\nPNL per trade:\\n%s\\n\" % (pnl_stats_w_cost['turnover'], pnl_stats_w_cost['pnl_per_trade']))\n",
    "\n",
    "    print(\"SR before cost:\\n\", pnl_stats['sharpe'])\n",
    "    print(pnl_stats['asset_sharpe_stats'])\n",
    "    print(\"Turnover: \\n%s\\nPNL per trade:\\n%s\\n\" % (pnl_stats['turnover'], pnl_stats['pnl_per_trade']))\n",
    "\n",
    "    iplot(pnl_stats_w_cost['portfolio_cumpnl'], title='portfolio pnl with cost')\n",
    "    iplot(pnl_stats_w_cost['asset_cumpnl'], title='asset pnl with cost')\n",
    "\n",
    "    iplot(pnl_stats['portfolio_cumpnl'], title='portfolio pnl wo cost')\n",
    "    iplot(pnl_stats['asset_cumpnl'], title='asset pnl wo cost')\n",
    "\n",
    "corr_cutoff = '2017-01-01'\n",
    "print(\"std:\\n%s\\n\" % pnl_df[corr_cutoff:].std(axis=0))\n",
    "\n",
    "pnl_w_df = pnl_df.drop(columns=['combo']).resample('W').sum()\n",
    "print(\"corr:\\n%s\\n\" % pnl_w_df[corr_cutoff:].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbdcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96700a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypfopt\n",
    "from pypfopt import plotting\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "dpnl = pnl_df['2017-01-01':].drop(columns=['combo'])\n",
    "strat_std = dpnl.std()\n",
    "dpnl = dpnl/dpnl.std()\n",
    "\n",
    "max_sharpe = False\n",
    "\n",
    "if max_sharpe:\n",
    "    mu = expected_returns.mean_historical_return(dpnl, returns_data=True, frequency=244, compounding=False)\n",
    "else:\n",
    "    mu = None\n",
    "\n",
    "S = risk_models.CovarianceShrinkage(dpnl, returns_data=True, frequency=244).ledoit_wolf()\n",
    "ef = EfficientFrontier(mu, S, weight_bounds= (0, 1)) \n",
    "\n",
    "fact_idx = {}\n",
    "for fact in dpnl.columns:\n",
    "    fact_idx[fact] = ef.tickers.index(fact)\n",
    "\n",
    "#ef.add_constraint(lambda w: w[fact_idx['ryield_ema_ts']] + w[fact_idx['ryield_ema_xdemean']] >= 0.3)\n",
    "#ef.add_constraint(lambda w: w[fact_idx['ryield_ema_ts']] + w[fact_idx['ryield_ema_xdemean']] <= 0.4)\n",
    "#ef.add_constraint(lambda w: w[fact_idx['ryield_ema_ts']] + w[fact_idx['ryield_ema_xdemean']] >= 0.3)\n",
    "# ef.add_constraint(lambda w: w[fact_idx['tsmom']] + w[fact_idx['macro2']] <= 0.24)\n",
    "\n",
    "#ef = EfficientFrontier(mu, S, weight_bounds=(0,1))\n",
    "if max_sharpe:\n",
    "    port_weights = ef.max_sharpe()\n",
    "else:\n",
    "    port_weights = ef.min_volatility()\n",
    "\n",
    "#port_weights = ef.clean_weights()\n",
    "port_weights = pd.Series(port_weights)\n",
    "print(\"port_weights=\\n%s\\n\" % port_weights)\n",
    "final_weights = port_weights.div(strat_std)\n",
    "print(\"final_weights=\\n%s\\n\" % final_weights)\n",
    "# mu = expected_returns.mean_historical_return(df)\n",
    "# S = risk_models.sample_cov(df)\n",
    "\n",
    "# # Optimize for maximal Sharpe ratio\n",
    "# ef = EfficientFrontier(mu, S)\n",
    "# weights = ef.max_sharpe()\n",
    "# ef.portfolio_performance(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_pnl[key]['portfolio_cumpnl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ec4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_lag_config = {\n",
    "    'll_left': -20,\n",
    "    'll_right': 120,\n",
    "    'll_spacing': 5,\n",
    "    'll_sub_win': [(datetime.date(2008, 1, 1), datetime.date(2018, 12, 31)), \n",
    "                   (datetime.date(2019, 1, 1), datetime.date(2024, 12, 31)),],\n",
    "}\n",
    "\n",
    "ll_keys = ['fullsample'] + ['%s:%s' % (sd.strftime('%Y-%b-%d'), ed.strftime('%Y-%b-%d')) for sd, ed in lead_lag_config['ll_sub_win']]\n",
    "\n",
    "\n",
    "ll_left = lead_lag_config['ll_left']\n",
    "ll_right = lead_lag_config['ll_right']\n",
    "spacing = lead_lag_config['ll_spacing']\n",
    "\n",
    "leadlag_df = bt_metrics.lead_lag(ll_limit_left=ll_left, \n",
    "                                 ll_limit_right=ll_right,\n",
    "                                 ll_sub_windows=lead_lag_config['ll_sub_win'])\n",
    "\n",
    "fig, ax = plt.subplots(len(ll_keys), 1)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "for i, key in enumerate(ll_keys):\n",
    "    ts = leadlag_df['leadlag_sharpes'].loc[key]\n",
    "    ts.plot(kind='bar', ax = ax[i], title = f'lead_lag: {key}')\n",
    "    new_ticks = np.linspace(ll_left, ll_right, (ll_right-ll_left)//spacing+1)\n",
    "    ax[i].set_xticks(np.interp(new_ticks, ts.index, np.arange(ts.size)))\n",
    "    ax[i].set_xticklabels(new_ticks)\n",
    "    ax[i].axvline(x=-ll_left, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ls_pnl = bt_metrics.long_short_pnl()\n",
    "for key in ls_pnl:\n",
    "    ax.plot(ls_pnl[key]['portfolio_cumpnl'], '-', label=key)\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, bbox_to_anchor=(1.04, 1), loc='upper left')\n",
    "ax.grid()\n",
    "plt.title(\"long-short pnl\")\n",
    "plt.show()\n",
    "\n",
    "lagged = bt_metrics.lagged_pnl(lags=[1, 5, 10, 20, 30, 60, 75, 80])\n",
    "lagged['cumpnl'].plot()\n",
    "#print('lagged PNL\\n', lagged['sharpe'])\n",
    "plt.grid()\n",
    "plt.title('lagged pnl')\n",
    "plt.show()\n",
    "\n",
    "smoothed = bt_metrics.smoothed_pnl(smooth_hls=[1, 5, 10, 20, 30, 60, 75, 80])\n",
    "smoothed['cumpnl'].plot(figsize=(8, 6))\n",
    "#print('smoothed PNL\\n', smoothed['sharpe'])\n",
    "plt.grid()\n",
    "plt.title('smoothed pnl')\n",
    "plt.show()\n",
    "\n",
    "#tilt_timing = bt_metrics.tilt_timing(tilt_rolling_window=1*244) # default 3 years  tilt_rolling_window = 3 * 244 \n",
    "\n",
    "seasonal_pnl = bt_metrics.seasonal_pnl()\n",
    "cumpnl = seasonal_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "#print('seasonal sharpe stats\\n', seasonal_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('monthly pnl')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "monthday_pnl = bt_metrics.monthday_pnl()\n",
    "cumpnl = monthday_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "#print('monthday sharpe stats\\n', monthday_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('monthday pnl')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "week_pnl = bt_metrics.week_pnl()\n",
    "cumpnl = week_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "#print('week sharpe stats\\n', week_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('weekday pnl')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "annual_pnl = bt_metrics.annual_pnl()\n",
    "cumpnl = annual_pnl['cumlog_pnl']\n",
    "cumpnl.set_index(cumpnl.index.astype('str')).plot(rot=30, figsize = (8, 6))\n",
    "#print('annual sharpe stats\\n', annual_pnl['sharpe_stats'])\n",
    "plt.grid()\n",
    "plt.title('annual pnl')\n",
    "plt.show()\n",
    "\n",
    "annual_pnl['cumlog_pnl'].mean(axis=1).plot()\n",
    "plt.grid()\n",
    "plt.title('annual averaged profile')\n",
    "plt.show()\n",
    "\n",
    "# turnover = bt_metrics.turnover()\n",
    "# print(turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a0123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cbd51da",
   "metadata": {},
   "source": [
    "# batch feature exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c12fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "#     'margin_hrc_sh', \n",
    "    'strip_hsec',\n",
    "    'strip_3.0x685',\n",
    "    'pipe_1.5x3.25',    \n",
    "    'hrc_sh',\n",
    "    'crc_sh',\n",
    "    'billet_ts',\n",
    "    'macf_cfd',\n",
    "#     'gi_0.5_sh',\n",
    "#     'hsec_400x200',\n",
    "#     'highwire_6.5',\n",
    "#     'angle_50x5',\n",
    "#     'ibeam_25',\n",
    "#     'channel_16',\n",
    "\n",
    "#     'import_arb', 'pbf_prem', 'plt65_62',\n",
    "#     'io_laytime_45ports', 'io_inv_imp_31ports',\n",
    "#     'io_invdays_imp_mill(64)', 'io_inv_mill(64)', 'io_inv_imp_mill',\n",
    "#     'io_removal_port_41',\n",
    "#     'io_loading_14ports_ausbzl',\n",
    "]\n",
    "\n",
    "udf = spot_df[feature_list].dropna(how='all')\n",
    "# lunar_seasonal = True\n",
    "\n",
    "# if lunar_seasonal:\n",
    "#     seasonal_signal = tstool.lunar_label(udf)\n",
    "#     seasonal_signal = tstool.seasonal_group_score(\n",
    "#         seasonal_signal, score_cols=feature_list, yr_col='lunar_cny',\n",
    "#         group_col='lunar_wks', min_obs=3, backward=2, forward=2, rolling_years=3)\n",
    "#     seasonal_signal = seasonal_signal.reindex(index=df.index).ffill()\n",
    "\n",
    "for feature in udf.columns:\n",
    "    dataseries.plot_seasonal_df(udf[feature].dropna(), cutoff='2018-01-01', title=feature)\n",
    "    \n",
    "signal_raw = udf[feature_list].reindex(index=df.index).ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = '2012-01-01'\n",
    "signal_func = 'qtl'\n",
    "param_rng = [20, 42, 2]\n",
    "signal_cap = None # [-2, 2]\n",
    "product_list = ['rb', 'hc', 'i', 'j', 'jm', 'SF', 'FG', ] # 'v', 'cu', 'al', 'ss', 'UR', 'SA', 'ru'\n",
    "\n",
    "for asset in product_list:\n",
    "    if '_' in asset:\n",
    "        price_ts = (1 + beta_ret_dict[asset]).cumprod().to_frame('price')[cutoff:]\n",
    "    else:\n",
    "        price_ts = df[(asset, 'c1', 'close')].dropna().to_frame('price')[cutoff:]\n",
    "    pnl_list = [price_ts]\n",
    "    for feature in feature_list:\n",
    "        feature_ts = udf[feature].reindex(index=price_ts.index).ffill()\n",
    "        #feature_ts = feature_ts.pct_change(5)\n",
    "        #feature_ts = tstool.lunar_yoy(feature_ts, group_col='lunar_days', func='pct_change')\n",
    "        #feature_ts = tstool.seasonal_score(feature_ts.to_frame())\n",
    "        signal_ts = calc_conv_signal(feature_ts, signal_func=signal_func, param_rng=param_rng, signal_cap=signal_cap)\n",
    "        asset_df = pd.concat([price_ts, signal_ts], axis=1)\n",
    "        asset_df.columns = ['price', 'signal']\n",
    "        asset_df['signal'] = asset_df['signal'].apply(lambda x: x).ffill()\n",
    "        asset_df = asset_df.dropna(subset=['price'])\n",
    "        asset_df['position'] = (asset_df['signal']/asset_df['price'].pct_change().rolling(20).std()).shift(1).fillna(0)\n",
    "        asset_df['pnl'] = (asset_df['position'].shift(1) * asset_df['price'].pct_change()).fillna(0)\n",
    "        \n",
    "        sr = np.sqrt(244) * asset_df['pnl'].mean()/asset_df['pnl'].std()\n",
    "        pnl_per_trade = 100 * 100 * asset_df['pnl'].mean()/asset_df['position'].diff().abs().mean()\n",
    "        turnover = 100 * asset_df['position'].diff().abs().mean()/asset_df['position'].abs().mean()\n",
    "        print(f'{asset}:{feature} -> SR: {sr:.2f} -- PNL per trade: {pnl_per_trade:.2f} -- Turnover: {turnover:.2f}')\n",
    "        pnl_list.append(asset_df['pnl'].cumsum().to_frame(feature))\n",
    "    pnl_df = pd.concat(pnl_list, axis=1)\n",
    "    dataseries.plot_df_on_2ax(pnl_df, left_on=feature_list, right_on=['price'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a57d7f",
   "metadata": {},
   "source": [
    "# signal grid search run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_list = []\n",
    "\n",
    "for feature in feature_list:\n",
    "    for win in [20, 40, 60, 80, 120, 240]:\n",
    "        signal_name = f\"{feature}:ma:{win}\"\n",
    "        signal_raw[signal_name] = signal_raw[feature] - signal_raw[feature].rolling(win).mean()\n",
    "        signal_raw[signal_name] = dh.risk_normalized(signal_raw[signal_name], 60)\n",
    "        signal_list.append(signal_name)\n",
    "        \n",
    "        signal_name = f\"{feature}:ewmac:{win}\"\n",
    "        signal_raw[signal_name] = dh.ewmac(signal_raw[feature], win_s=win/10, ls_ratio=2)\n",
    "        signal_raw[signal_name] = dh.risk_normalized(signal_raw[signal_name], 60)\n",
    "        signal_list.append(signal_name)\n",
    "\n",
    "#         signal_name = f\"{feature}:convewm:{win}\"\n",
    "#         signal_raw[signal_name] = dh.conv_ewm(signal_raw[feature], h1s=[win//10, win//10*2], h2s=[win//10*3, win//10*6])\n",
    "#         signal_raw[signal_name] = dh.risk_normalized(signal_raw[signal_name], 60)\n",
    "#         signal_list.append(signal_name)\n",
    "        \n",
    "        signal_name = f\"{feature}:zscore:{win}\"\n",
    "        signal_raw[signal_name] = dh.zscore_roll(signal_raw[feature], win=win)\n",
    "        signal_list.append(signal_name) \n",
    "        \n",
    "        signal_name = f\"{feature}:zscore_dff20:{win}\"\n",
    "        signal_raw[signal_name] = dh.zscore_roll(signal_raw[feature].diff(20), win=win)\n",
    "        signal_list.append(signal_name) \n",
    "        \n",
    "        signal_name = f\"{feature}:qtl:{win}\"\n",
    "        signal_raw[signal_name] = dh.pct_score(signal_raw[feature], win=win)*2\n",
    "        signal_list.append(signal_name) \n",
    "        \n",
    "        signal_name = f\"{feature}:qtl_dff20:{win}\"\n",
    "        signal_raw[signal_name] = dh.pct_score(signal_raw[feature].diff(20), win=win)*2\n",
    "        signal_list.append(signal_name)\n",
    "        \n",
    "        signal_name = f\"{feature}:lunar_wks_score:{win}\"\n",
    "        signal_raw[signal_name] = seasonal_signal[feature]\n",
    "        signal_list.append(signal_name)\n",
    "        \n",
    "        signal_prefix = f\"{feature}:seasonal_score\"\n",
    "        signal_raw[signal_prefix] = tstool.seasonal_score(signal_raw[feature].to_frame())\n",
    "        signal_name = f\"{signal_prefix}_pct:{win}\"\n",
    "        signal_raw[signal_name] = dh.pct_score(signal_raw[feature], win=win)*2\n",
    "        signal_list.append(signal_name) \n",
    "        signal_name = f\"{signal_prefix}_zscore:{win}\"\n",
    "        signal_raw[signal_name] = dh.zscore_roll(signal_raw[feature], win=win)\n",
    "        signal_list.append(signal_name)\n",
    "        \n",
    "        signal_prefix = f\"{feature}:yoy\"\n",
    "        signal_raw[signal_prefix] = signal_raw[feature]/signal_raw[feature].shift(244)-1\n",
    "        signal_name = f\"{signal_prefix}_pct:{win}\"\n",
    "        signal_raw[signal_name] = dh.pct_score(signal_raw[feature], win=win)*2\n",
    "        signal_list.append(signal_name) \n",
    "        signal_name = f\"{signal_prefix}_zscore:{win}\"\n",
    "        signal_raw[signal_name] = dh.zscore_roll(signal_raw[feature], win=win)\n",
    "        signal_list.append(signal_name)\n",
    "\n",
    "        signal_prefix = f\"{feature}:lunar_yoy\"\n",
    "        signal_raw[signal_prefix] = tstool.lunar_yoy(signal_raw[feature], group_col='lunar_days', func='pct_change')\n",
    "        signal_name = f\"{signal_prefix}_pct:{win}\"\n",
    "        signal_raw[signal_name] = dh.pct_score(signal_raw[feature], win=win)*2\n",
    "        signal_list.append(signal_name) \n",
    "        signal_name = f\"{signal_prefix}_zscore:{win}\"\n",
    "        signal_raw[signal_name] = dh.zscore_roll(signal_raw[feature], win=win)\n",
    "        signal_list.append(signal_name)\n",
    "        \n",
    "signal_raw = signal_raw.reindex(index=df.index).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = ['rb', 'hc', 'i', 'j', 'jm', 'v', 'FG', 'SM', 'SF']\n",
    "cutoff = pd.Timestamp('2012-07-01')\n",
    "\n",
    "for sig in signal_list:\n",
    "    print(sig)\n",
    "    pnl_by_asset = {}\n",
    "    pnl_df = pd.DataFrame()\n",
    "    pos_df = pd.DataFrame()\n",
    "    for asset in product_list:\n",
    "        signal = signal_raw[sig]\n",
    "        asset_df = pd.concat([df[(asset, 'c1', 'close')], signal], axis=1)\n",
    "        asset_df.columns = ['price', 'signal']\n",
    "        asset_df['signal'] = asset_df['signal'].apply(lambda x: x).ffill()\n",
    "        asset_df = asset_df.dropna(subset=['price']).ffill()\n",
    "        asset_df['position'] = (asset_df['signal']/asset_df['price'].pct_change().rolling(20).std()).shift(1).fillna(0)\n",
    "        asset_df['pnl'] = (asset_df['position'].shift(1) * asset_df['price'].pct_change()).fillna(0)\n",
    "        \n",
    "        sr = np.sqrt(244) * asset_df['pnl'].mean()/asset_df['pnl'].std()\n",
    "        pnl_per_trade = 100 * 100 * asset_df['pnl'].mean()/asset_df['position'].diff().abs().mean()\n",
    "        turnover = 100 * asset_df['position'].diff().abs().mean()/asset_df['position'].abs().mean()\n",
    "        print(f'{asset} -> SR: {sr:.2f} -- PNL per trade: {pnl_per_trade:.2f} -- Turnover: {turnover:.2f}')\n",
    "        \n",
    "        pnl_by_asset[asset] = asset_df\n",
    "        pnl_df[asset] = asset_df['pnl']\n",
    "        pos_df[asset] = asset_df['position']\n",
    "    pnl_df = pnl_df.fillna(0)\n",
    "    pos_df = pos_df.ffill()\n",
    "    total_sr = = np.sqrt(244) * pnl_df.sum(axis=1).mean()/pnl_df.sum(axis=1).std()\n",
    "    print(f'Total SR: {total_sr:.2f}')\n",
    "    \n",
    "    cumpn; = pnl_df.cumsum()\n",
    "    cumpnl.plot()\n",
    "    plt.title(sig)\n",
    "    plt.show()\n",
    "    cum_pnl.sum(axis=1).plot()\n",
    "    plt.title(sig)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2e65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730f831a",
   "metadata": {},
   "source": [
    "# Signal portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_dict_full = {\n",
    "    'i': [\n",
    "        ('io_removal_lvl_fast', 1.0), \n",
    "        #('io_removal_lyoy_mom', 1.0),\n",
    "        \n",
    "        ('io_inv_mill(64)_lvl_fast', 0.5),\n",
    "        #('io_inv_mill(64)_lyoy_mom', 0.5),\n",
    "        \n",
    "        ('io_invdays_imp_mill(64)_lvl_fast', 0.5),\n",
    "        #('io_invdays_imp_mill(64)_lyoy_mom', 0.5),\n",
    "        \n",
    "#         ('steel_social_inv_lvl_fast', 1.0/1.0),\n",
    "#         ('rebar_inv_social_lyoy_fast', 0.25/1.0),\n",
    "#         ('wirerod_inv_social_lyoy_fast', 0.25/1.0),\n",
    "#         ('hrc_inv_social_lyoy_fast', 0.25/1.0),\n",
    "#         ('crc_inv_social_lyoy_fast', 0.25/1.0),\n",
    "        \n",
    "        ('margin_lvl_fast', 1.0),\n",
    "        ('strip_hsec_lvl_mid', 1.0),\n",
    "        ('macf_cfd_lvl_mid', 1.0),\n",
    "#         ('pbf_prem_yoy', 0.5/15),\n",
    "#         ('cons_steel_lyoy_slow', 1.0/1.5),\n",
    "#         ('sea_export_arb_lvl_mid', 1.0/1.4),\n",
    "    ],\n",
    "    'rb': [\n",
    "        ('io_removal_lvl_fast', 1.0), \n",
    "        #('io_removal_lyoy_mom', 1.0),\n",
    "        \n",
    "        ('io_inv_mill(64)_lvl_fast', 1.0),\n",
    "        #('io_inv_mill(64)_lyoy_mom', 1.0),\n",
    "        \n",
    "#         ('rebar_inv_social_lyoy_fast', 1.0),\n",
    "#         ('wirerod_inv_social_lyoy_fast', 1.0),\n",
    "        \n",
    "        ('margin_lvl_fast', 1.0),\n",
    "        ('strip_hsec_lvl_mid', 1.0),    \n",
    "    ],\n",
    "    'hc': [\n",
    "        ('io_removal_lvl_fast', 1.0), \n",
    "        #('io_removal_lyoy_mom', 1.0),\n",
    "        \n",
    "        ('io_inv_mill(64)_lvl_fast', 1.0),\n",
    "        #('io_inv_mill(64)_lyoy_mom', 1.0),\n",
    "        \n",
    "#         ('hrc_inv_social_lyoy_fast', 1.0),\n",
    "#         ('crc_inv_social_lyoy_fast', 1.0),        \n",
    "        ('margin_lvl_fast', 1.0),\n",
    "        ('strip_hsec_lvl_mid', 1.0),  \n",
    "    ],\n",
    "    'j': [\n",
    "        #('io_removal_lvl_fast', 1.0), \n",
    "        #('io_removal_lyoy_mom', 1.0),\n",
    "        \n",
    "        #('io_inv_mill(64)_lvl_fast', 1.0),\n",
    "        #('io_inv_mill(64)_lyoy_mom', 1.0),\n",
    "        \n",
    "#         ('steel_social_inv_lvl_fast', 1.0),\n",
    "        \n",
    "        ('margin_lvl_fast', 1.0),\n",
    "        ('strip_hsec_lvl_mid', 1.0),    \n",
    "    ],\n",
    "    'jm': [\n",
    "        #('io_inv_mill(64)_lvl_fast', 1.0),\n",
    "        #('io_inv_mill(64)_lyoy_mom', 1.0),\n",
    "        \n",
    "#         ('steel_social_inv_lvl_fast', 1.0),\n",
    "        \n",
    "        ('margin_lvl_fast', 1.0),\n",
    "        ('strip_hsec_lvl_mid', 1.0),     \n",
    "    ],\n",
    "    'FG': [\n",
    "        #('io_inv_mill(64)_lvl_fast', 1.0),\n",
    "        #('io_inv_mill(64)_lyoy_mom', 1.0),\n",
    "        \n",
    "        ('margin_lvl_fast', 1.0),\n",
    "        ('strip_hsec_lvl_mid', 1.0),    \n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe34dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_dict = signal_dict_full\n",
    "\n",
    "signal_diagnosis = False\n",
    "\n",
    "pnl_dict = {}\n",
    "pos_dict = {}\n",
    "\n",
    "for asset in ['i', 'rb', 'hc', 'j', 'jm']:\n",
    "    if '_' in asset:\n",
    "        price_ts = (1 + beta_ret_dict[asset]).cumprod().to_frame('price')\n",
    "    else:\n",
    "        price_ts = df[(asset, 'c1', 'close')].dropna().to_frame('price')\n",
    "    pnl_list = []\n",
    "    pos_list = []\n",
    "    for idx, (feature_name, weight) in enumerate(signal_dict[asset]):\n",
    "        feature, signal_func, param_rng, proc_func, chg_func, bullish, freq = signal_repo[feature_name]\n",
    "        if freq == 'price':\n",
    "            feature_ts = spot_df[feature].ffill().reindex(index=price_ts.index).ffill()\n",
    "        elif len(freq) > 0:\n",
    "            feature_ts = spot_df[feature].ffill().reindex(index=pd.date_range(start=df.index[0], end=df.index[-1], freq=freq)).ffill()\n",
    "        else:\n",
    "            feature_ts = spot_df[feature].dropna()\n",
    "        \n",
    "        if 'yoy' in proc_func:\n",
    "            if 'lunar' in proc_func:\n",
    "                label_func = lunar_label\n",
    "                label_args = {}\n",
    "            else:\n",
    "                label_func = calendar_label\n",
    "                label_args = {'anchor_date': {'month': 1, 'day': 1}}\n",
    "            if '_wk' in proc_func:\n",
    "                group_col = 'label_wk'\n",
    "            else:\n",
    "                group_col = 'label_day'\n",
    "            feature_ts = yoy_generic(feature_ts, label_func=label_func, group_col='label_day', func=chg_func, label_args=label_args)\n",
    "        elif 'df' in proc_func:\n",
    "            n_diff = int(proc_func[2:])\n",
    "            feature_ts = getattr(feature_ts, chg_func)(n_diff)\n",
    "    \n",
    "        if signal_func == 'seasonal_score_w':\n",
    "            signal_ts = seasonal_score(feature_ts.to_frame(), backward=10, forward=10, rolling_years=3, min_obs=10).reindex(index=df.index).ffill()\n",
    "        elif signal_func == 'seasonal_score_d':\n",
    "            signal_ts = seasonal_score(feature_ts.to_frame(), backward=15, forward=15, rolling_years=3, min_obs=30)\n",
    "        elif len(signal_func)>0:\n",
    "            feature_ts = feature_ts.reindex(index=df.index).ffill()\n",
    "            signal_ts = calc_conv_signal(feature_ts, signal_func=signal_func, param_rng=param_rng, signal_cap=signal_cap)\n",
    "        else:\n",
    "            signal_ts = feature_ts.reindex(index=df.index).ffill()\n",
    "            \n",
    "        if not bullish:\n",
    "            signal_ts = -signal_ts\n",
    "            \n",
    "        asset_df = pd.concat([price_ts, signal_ts], axis=1)\n",
    "        asset_df.columns = ['price', 'signal']\n",
    "        asset_df['signal'] = asset_df['signal'].apply(lambda x: x).ffill()\n",
    "        asset_df = asset_df.dropna(subset=['price'])\n",
    "        asset_df['position'] = (weight*asset_df['signal']/asset_df['price'].pct_change().rolling(20).std()).shift(1).fillna(0)\n",
    "        asset_df['pnl'] = (asset_df['position'].shift(1) * asset_df['price'].pct_change()).fillna(0)\n",
    "        \n",
    "        std = asset_df['pnl'].std()\n",
    "        sr = np.sqrt(244) * asset_df['pnl'].mean()/asset_df['pnl'].std()\n",
    "        pnl_per_trade = 100 * 100 * asset_df['pnl'].mean()/asset_df['position'].diff().abs().mean()\n",
    "        turnover = 100 * asset_df['position'].diff().abs().mean()/asset_df['position'].abs().mean()\n",
    "        print(f'{asset}:{feature_name} -> SR: {sr:.2f} -- PNL per trade: {pnl_per_trade:.2f} -- Turnover: {turnover:.2f}')\n",
    "        pnl_list.append(asset_df['pnl'].to_frame(feature_name))\n",
    "        pos_list.append(asset_df['position'].to_frame(feature_name))\n",
    "        \n",
    "    pnl_df = pd.concat(pnl_list, axis=1)\n",
    "    pos_df = pd.concat(pos_list, axis=1)\n",
    "    sum_pnl = pnl_df.sum(axis=1)\n",
    "    sum_pos = pos_df.sum(axis=1)\n",
    "    sr = np.sqrt(244) * sum_pnl.mean()/sum_pnl.std()\n",
    "    pnl_per_trade = 100 * 100 * sum_pnl.mean()/sum_pos.diff().abs().mean()\n",
    "    turnover = 100 * sum_pos.diff().abs().mean()/sum_pos.abs().mean()\n",
    "    print(f'{asset}:total -> SR: {sr:.2f} -- PNL per trade: {pnl_per_trade:.2f} -- Turnover: {turnover:.2f}')\n",
    "    \n",
    "    print(pnl_df.std())\n",
    "    pnl_dict[asset] = pnl_df\n",
    "    pos_dict[asset] = pos_df\n",
    "    pnl_df.cumsum().plot()\n",
    "    plt.show()\n",
    "    sum_pnl.cumsum().plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dceab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8962a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_backward = False\n",
    "smooth_win = 1\n",
    "sig_smooth = tstool.exp_smooth(df_in, hl = smooth_win, fill_backward=fill_backward)\n",
    "\n",
    "demean = False\n",
    "mean_win = 244\n",
    "vol_win = 244\n",
    "if demean:\n",
    "    sig_scored = tstool.ts_score(sig_smooth, hl_mean=mean_win, min_obs_mean=mean_win, fill_backward_mean=fill_backward, \n",
    "                          hl_vol=vol_win, min_obs_vol=vol_win, fill_backward_vol=fill_backward)\n",
    "else:\n",
    "    sig_scored = tstool.ts_scale(sig_smooth, hl = vol_win, min_obs=vol_win, fill_backward=fill_backward)\n",
    "\n",
    "#sig_scored = tstool.xs_score(sig_smooth, demean=demean, hl=vol_win)\n",
    "\n",
    "signal_cap = 2.0\n",
    "\n",
    "score_capped = tstool.cap(sig_scored, -signal_cap, signal_cap)\n",
    "score_filled = tstool.filldown(score_capped, 2)\n",
    "score = tstool.lag(score_filled, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_scale = 20\n",
    "asset_vol = tstool.exp_smooth(df_pxchg**2, hl=vol_scale, fill_backward=fill_backward)**0.5\n",
    "holding = score/asset_vol\n",
    "\n",
    "commod_list = holding.columns #['hc']\n",
    "btmetrics = MetricsBase(holdings = holding[commod_list], returns = df_pxchg[commod_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.623px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
