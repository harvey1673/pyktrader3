{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "PNL_BDAYS = 252\n",
    "\n",
    "#def exp_smooth(df_in, hl, min_obs = None, max_window = None, fill_backward = True, fill_forward = False):\n",
    "\n",
    "def cap(df_in, min_val, max_val):\n",
    "    df_out = df_in.copy()\n",
    "    df_out[df_out < min_val] = min_val\n",
    "    df_out[df_out > max_val] = max_val\n",
    "    \n",
    "\n",
    "def lag(df_in, lag):\n",
    "    df_out = df_in.shift(lag)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def filldown(df_in, maxfill = 1):\n",
    "    df_out = df_in.fillna(method = 'ffill', limit = maxfill)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def fillup(df_in, maxfill = 1):\n",
    "    df_out = df_in.fillna(method = 'bfill', limit = maxfill)\n",
    "    return df_out\n",
    "\n",
    "def exp_smooth(df_in, hl, min_obs = 0, fill_backward = True):\n",
    "    df_out = df_in.ewm(halflife = hl,  min_periods=min_obs).mean()\n",
    "    if fill_backward:\n",
    "        df_out = df_out.fillna(method = 'bfill')\n",
    "    return df_out\n",
    "\n",
    "def ts_demean(df_in, hl, min_obs = 0, fill_backward = True):\n",
    "    means = exp_smooth(df_in, hl, min_obs = min_obs, fill_backward = fill_backward)\n",
    "    df_out = df_in - means\n",
    "    return df_out\n",
    "\n",
    "def ts_scale(df_in, hl, min_obs = 0, fill_backward = True):\n",
    "    vars = df_in.pow(2.0)\n",
    "    vars_sm = exp_smooth(vars, hl, min_obs = min_obs, fill_backward = fill_backward)\n",
    "    vols_sm = vars_sm.pow(0.5)\n",
    "    df_out = df_in/vols_sm\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def ts_score(df_in, hl_mean, hl_vol, min_obs_mean = 0, fill_backward_mean = True,\n",
    "            min_obs_vol = 0, fill_backward_vol = True):\n",
    "    df_demean = ts_demean(df_in, hl_mean, min_obs = min_obs_mean, fill_backward = fill_backward_mean)\n",
    "    df_out = ts_scale(df_demean, hl_vol, min_obs = min_obs_vol, fill_backward = fill_backward_vol)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def np_nanmean_nowarning(nd_in, axis = 0, keepdims = True):\n",
    "    import warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category = warnings.RuntimeWarnings)\n",
    "        means = np.nanmean(nd_in, axis = axis, keepdims = keepdims)\n",
    "    return means\n",
    "\n",
    "\n",
    "def clip_by_rolling_std(df_in, std_cut = 3, **kwargs):\n",
    "    rolling_std = df_in.rolling(**kwargs).std()\n",
    "    rolling_mean = df_in.rolling(**kwargs).mean()\n",
    "    upper = rolling_mean + std_cut * rolling_std\n",
    "    lower = rolling_mean - std_cut * rolling_std\n",
    "    df_in = df_in.reindex(upper.dropna().index)\n",
    "    return df_in.clip(lower = lower.dropna(), upper = upper.dropna(), axis = 0)\n",
    "\n",
    "\n",
    "def xs_mean(df_in):\n",
    "    means = np_nanmean_nowarning(df_in.values, axis = 1)\n",
    "    series_out = pd.Series(means, index = df_in.index)\n",
    "    return series_out\n",
    "\n",
    "\n",
    "def xs_mean_repeat(df_in):\n",
    "    means = np_nanmean_nowarning(df_in.values, axis = 1)\n",
    "    mean_repeat = np.repeat(means, len(df_in.columns), axis = 1)\n",
    "    mean_repeat[np.isnan(df_in.values)] = np.NaN\n",
    "    df_out = pd.DataFrame(mean_repeat, index = df_in.index, columns = df_in.columns)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def xs_demean(df_in):\n",
    "    df_out = df_in - xs_mean_repeat(df_in)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def xs_score(df_in, demean = True, hl = None):\n",
    "    if demean:\n",
    "        df_demeaned = xs_demean(df_in)\n",
    "    else:\n",
    "        df_demeaned = df_in\n",
    "    vols_raw = df_demeaned.std(axis=1)\n",
    "    if hl is None:\n",
    "        vols = vols_raw\n",
    "    else:\n",
    "        vols = exp_smooth(vols_raw, hl)\n",
    "        \n",
    "    data_scored = df_demeaned.values/np.repeat(vols.values.resahpe([len(vols.index),1]), \n",
    "                                               len(df_in.columns), axis = 1)\n",
    "    df_out = pd.dataFframe(data_scored, index = df_in.index, columns = df_in.columns)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def seasonal_helper(df_in, func, data_range = None, min_obs = 0, \n",
    "                    backward = 30, forward = 30, rolling_years = 100, **kwargs):\n",
    "    import datetime as dt\n",
    "    import copy\n",
    "    df_in = copy.deepcopy(df_in)\n",
    "    results = {}\n",
    "    if date_range is None:\n",
    "        date_range = df_in.index\n",
    "        \n",
    "    for t_date in date_range:\n",
    "        try:\n",
    "            past_years = set(df_in[:t_date].index.year)\n",
    "            mask = []\n",
    "            \n",
    "            for y in past_years:\n",
    "                if t_date.year - y > rolling_years:\n",
    "                    continue\n",
    "                \n",
    "                if t_date.month == 2 and t_date.day == 29:\n",
    "                    start = t_date.replace(year = y, day = 28) - dt.timedelta(days = backward)\n",
    "                else:\n",
    "                    start = t_date.replace(year = y) - dt.timedelta(days = backward)\n",
    "                    \n",
    "                if y == t_date.year:\n",
    "                    end = t_date\n",
    "                else:\n",
    "                    end = start + dt.timedelta(days = backward) + dt.timedelta(days = forward)\n",
    "                mask.append((df_in.index >= start) & (df_in.index <= end))\n",
    "            sample_period = df_in.loc[reduce(np.logical_or, mask)]\n",
    "            if sample_period.empty or (min_obs is not None and len(sample_period) < min_obs): continue\n",
    "            results[t_date] = func(sample_period, **kwargs)\n",
    "        except Exception:\n",
    "            raise\n",
    "    return results\n",
    "\n",
    "\n",
    "def seasonal_score(signal_df, **kwargs):\n",
    "    def agg_func(sample_df):\n",
    "        return (sample_df.iloc[-1] - sample_df.mean())/sample_df.std()\n",
    "    df = seasonal_helper(df_in = signal_df, func = agg_func, **kwargs)\n",
    "    return pd.DataFrame(df).T.reindex_like(signal_df)\n",
    "\n",
    "\n",
    "def rolling_deseasonal(raw_df, **kwargs):\n",
    "    def agg_func(sample_df):\n",
    "        return sample_df.iloc[-1] - sample_df.mean()\n",
    "    \n",
    "    df = seasonal_helper(df_in = raw_df, func = agg_func, **kwargs)\n",
    "    return pd.DataFrame(df).T.reindex_like(raw_df)\n",
    "\n",
    "\n",
    "def get_sharpe(pnl):\n",
    "    return np.nanmean(pnl)/np.nanstd(pnl) * np.sqrt(PNL_BDAYS)\n",
    "\n",
    "\n",
    "def get_success_rate(pnl):\n",
    "    valid_pnl = pnl[pnl.abs()>0].dropna()\n",
    "    return len(valid_pnl[valid_pnl > 0]) / len(valid_pnl)\n",
    "\n",
    "\n",
    "def get_ema_diff_signal(price_adj, span_fast, span_slow, quantile_window = 250):\n",
    "    diff = price_adj.ewm(span = span_fast, min_periods = span_fast).mean() \\\n",
    "            - price_adj.ewm(span = span_slow, min_periods = span_fast).mean()\n",
    "    diff_quantile = get_rolling_percentiles(diff, window = quantile_window)\n",
    "    signal = (diff_quantile - 0.5) * 2\n",
    "    return signal\n",
    "\n",
    "\n",
    "def percentile(x, vector):\n",
    "    if np.isnan(x):\n",
    "        return np.nan\n",
    "    vector = np.array(vector)\n",
    "    vector = vector[~np.isnan(vector)]\n",
    "    return np.sum(vector<x) / len(vector)\n",
    "\n",
    "\n",
    "def get_rolling_percentiles(vector, window = 252, min_periods = None, use_abs = False):\n",
    "    if min_periods is None:\n",
    "        min_periods = window // 3 + 1\n",
    "    if not isinstance(vector, pd.Series):\n",
    "        vector = pd.Series(vector)\n",
    "        \n",
    "    if use_abs:\n",
    "        percentiles = vector.abs().rolling(window + 1, \n",
    "                                           min_peridos = min_periods).apply(lambda s: percentile(s[-1], s[:-1]), raw = True)\n",
    "        percentiles *= np.sign(vector)\n",
    "    else:\n",
    "        percentiles = vector.rolling(window + 1, \n",
    "                                     min_peridos = min_periods).apply(lambda s: percentile(s[-1], s[:-1]), raw = True)\n",
    "    return percentiles\n",
    "\n",
    "\n",
    "def get_scored_signal(signal, hl_smooth = 20, hl_score = 252, demean_signal = True):\n",
    "    sig_smooth = exp_smooth(signal, hl = hl_smooth)\n",
    "    if demean_signal:\n",
    "        sig_scored = ts_score(sig_smooth, hl_vol = hl_score, hl_mean = hl_score)\n",
    "    else:\n",
    "        sig_scored = ts_scale(sig_smooth, hl = hl_score)\n",
    "    score_capped = cap(sig_scored, -2, 2)\n",
    "    score_filled = filldown(score_capped, 2)\n",
    "    return score_filled\n",
    "\n",
    "\n",
    "def generate_signal_sensitivity_report(signals, pnls, quantiles = None, nb_bins = 6, p = 0.7, return_fig = False):\n",
    "    with sns.plotting_context('notebook'):\n",
    "        if quantiles is None:\n",
    "            quantiles = [0.1, 0.25, 0.5]\n",
    "        fig, axarray = plt.subplots(2,2,figsize = (12,8))\n",
    "        fig.subplots_adjust(hspace = 0.4, wspace = 0.25)\n",
    "        \n",
    "        colors1 = sns.color_palatte(\"Set1\", len(quantiles) + 2)\n",
    "        colors2 = sns.color_palatte(\"Set2\", 3)\n",
    "        \n",
    "        signals = signals.loc[pnls.index].copy()\n",
    "        \n",
    "        Q = pd.DataFrame(index = signals.index)\n",
    "        Q['100th Perc'] = pnls\n",
    "        for q in quantiles:\n",
    "            to_keep = signals[signals.abs() > signals.abs().quantile(q)]\n",
    "            Q[f'{(1 - q) * 100}th Perc'] = pnls[to_keep.index].copy()\n",
    "        unique_signals = signals.unique()\n",
    "        unique_signals.sort()\n",
    "        unique_signals = [unique_signals[0] - 1e-6] + unique_signals.tolist()\n",
    "        \n",
    "        if nb_bins < len(unique_signals):\n",
    "            binned = pd.qcut(signals, nb_bins, duplicates = 'drop')\n",
    "            \n",
    "            bin_delta = 0\n",
    "            while (len(binned.unique()) < nb_bins):\n",
    "                bin_delta += 1\n",
    "                binned = pd.qcut(signals, nb_bins + bin_delta, duplicates = 'drop')\n",
    "        else:\n",
    "            binned = pd.qcut(signals, unique_signals, duplicates = 'drop')\n",
    "        \n",
    "        df = pnls.copy().to_frame()\n",
    "        df.columns = ['PnL']\n",
    "        df['binned'] = binned\n",
    "        PnL_per_bin = df.groupby('binned').mean()['PNL']\n",
    "        Q25_per_bin = df.groupby('binned').quantile(0.25)['PNL']\n",
    "        Q75_per_bin = df.groupby('binned').quantile(0.75)['PNL']\n",
    "        bins = list(binned.cat.categories)\n",
    "        \n",
    "        ax = axarray[0, 0]\n",
    "        for i, col in enumerate(Q.columns):\n",
    "            sharpe = get_sharpe(Q[col])\n",
    "            sr = get_success_rate(Q[col])\n",
    "            cum_pnl = Q[col].cumsum().dropna()\n",
    "            if len(cum_pnl) > 0:\n",
    "                cum_pnl.plot(ax = ax, color = colors1[i], lavel = f'{col}, sharpe = {sharpe:.2f} - SR = {sr:.2f}')\n",
    "        ax.legend(loc = 'best', frameon = False)\n",
    "        ax.set_title('cumulative PnL signal dependence')\n",
    "        ax.set_ylabel('cumulative PnL')\n",
    "        \n",
    "        last_signal = signals.iloc[-1]\n",
    "        \n",
    "        ax = axarray[0, 1]\n",
    "        ax.bar(\n",
    "            x = np.arrange(len(PnL_per_bin)),\n",
    "            height = PnL_per_bin.values,\n",
    "            tick_label = [pd.Interval(np.round(i.left, 2), np.round(i.right, 2)) for i in bins],\n",
    "            yerr = (Q75_per_bin - Q25_per_bin).values,\n",
    "            color = [colors2[2] if last_signal in i else colors2[0] for i in bins]\n",
    "        )\n",
    "        ax.set_xticklabels(ax.xaxis.get_ticklabels(), rotation = 70, fontsize = 9)\n",
    "        ax.set_ylabel('average PnL')\n",
    "        \n",
    "        pvals_sharpe = df.groupby('binned')['PnL'].apply(get_sharpe)\n",
    "        for i, bbb in enumerate(ax.patches):\n",
    "            ax.annotate(f'{pvals_sharpe.iloc[i]:.2f}', (bbb.get_x(), PnL_per_bin.values[i] * 1.5), fontsize = 12)\n",
    "        \n",
    "        sub_strats = []\n",
    "        ns = int(p + len(pnls))\n",
    "        for _ in range(50):\n",
    "            sub_strats += [sklearn.utils.resample(pnls, n_samples = ns).sort_index().cumsum()]\n",
    "        \n",
    "        ax = axarray[1, 0]\n",
    "        PnL_cols = sns.cubehelix_palette(len(sub_strats))\n",
    "        ax.set_ylabel('subsample cumulative PnL', fontsize = 12)\n",
    "        \n",
    "        for cum_pnl, col in zip(sub_strats, PnL_cols):\n",
    "            ax.plot(cum_pnl, color = col, linewidth = 0.6)\n",
    "            \n",
    "        ax.set_xlim(pnls.index[0], pnls.index[-1])\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_totation(45)\n",
    "            \n",
    "        ax = axarray[1, 1]\n",
    "        abs_pnl = pnls.abs()\n",
    "        \n",
    "        if len(pnls) > 0:\n",
    "            pnls.cumsum().plot(ax = ax, color = 'black', label = f'full PnL', linewidth = 1)\n",
    "            for q_min, q_max in [(0, 20), (20, 40), (40, 60), (60, 80), (80, 100)]:\n",
    "                abs_pnl_min = abs_pnl[abs_pnl>0].quantile(q_min/100)\n",
    "                abs_pnl_max = abs_pnl[abs_pnl>0].quantile(q_max/100)\n",
    "                \n",
    "                pnl_filtered = pnls[(abs_pnl_min < abs_pnl) & (abs_pnl <= abs_pnl_max)]\n",
    "                pnl_filtered.cumsum().plot(ax = ax, label = f'abs(PnL) {q_min}-{q_max}%', linewidth = 1)\n",
    "        ax.set_ylabel('cumulative PnL', fontsize = 12)\n",
    "        ax.legend(loc = 'best', frameon = False)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    if return_fig:\n",
    "        return fig\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
